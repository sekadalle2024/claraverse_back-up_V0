# Auto-generated llama-swap configuration - FIXED VERSION
# Models directory: C:\Users\Admin\.clara\llama-models
healthCheckTimeout: 30
logLevel: info

models:
  "qwen-0-6b-coder-iq4-xs:0.6b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\Admin\.clara\llama-models\qwen-0.6b-coder.IQ4_XS.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  # ORIGINAL GEMMA CONFIG (DISABLED - CAUSING CUDA ERROR)
  # "gemma3:4b":
  #   proxy: "http://127.0.0.1:9999"
  #   cmd: |
  #     "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
  #     -m "C:\Users\BadBoy17G\gemma-3-4b-it-IQ4_NL.gguf"
  #     --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
  #   ttl: 300

  # TRY THIS FIRST - CPU ONLY (SAFEST OPTION)
  "gemma3:4b-cpu":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\BadBoy17G\gemma-3-4b-it-IQ4_NL.gguf"
      --port 9999 --jinja --n-gpu-layers 0 --threads 8 --ctx-size 4096 --batch-size 64 --ubatch-size 64 --keep 512 --defrag-thold 0.1 --mlock --parallel 1
    ttl: 300

  # TRY THIS SECOND - REDUCED GPU LAYERS
  "gemma3:4b-reduced":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\BadBoy17G\gemma-3-4b-it-IQ4_NL.gguf"
      --port 9999 --jinja --n-gpu-layers 20 --threads 8 --ctx-size 8192 --batch-size 128 --ubatch-size 128 --keep 1024 --defrag-thold 0.1 --mlock --parallel 1
    ttl: 300

  # TRY THIS THIRD - CONSERVATIVE GPU SETTINGS
  "gemma3:4b-conservative":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\BadBoy17G\gemma-3-4b-it-IQ4_NL.gguf"
      --port 9999 --jinja --n-gpu-layers 35 --threads 8 --ctx-size 4096 --batch-size 64 --ubatch-size 64 --keep 512 --defrag-thold 0.1 --mlock --parallel 1
    ttl: 300

  "llama3.2:1b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\BadBoy17G\Llama-3.2-1B-Instruct-IQ4_XS.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "nomic-embed-text-v1-5":
    proxy: "http://127.0.0.1:9998"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\BadBoy17G\nomic-embed-text-v1.5.Q4_0.gguf"
      --port 9998 --jinja --n-gpu-layers 50 --pooling mean --embeddings --threads 8 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "nomic-embed-text-v2":
    proxy: "http://127.0.0.1:9998"
    cmd: |
      "C:\Users\Admin\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\BadBoy17G\nomic-embed-text-v2-moe.Q8_0.gguf"
      --port 9998 --jinja --n-gpu-layers 50 --pooling mean --embeddings --threads 8 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300
 
groups:
  "embedding_models":
    # Allow multiple embedding models to run together
    swap: false
    # Don't unload other groups when embedding models start
    exclusive: false
    # Prevent other groups from unloading embedding models
    persistent: true
    members:
      - "nomic-embed-text-v1-5"
      - "nomic-embed-text-v2"

  "regular_models":
    # Only one regular model at a time (traditional behavior)
    swap: true
    # Unload other non-persistent groups when loading
    exclusive: true
    members:
      - "qwen-0-6b-coder-iq4-xs:0.6b"
      - "gemma3:4b-cpu"
      - "gemma3:4b-reduced"  
      - "gemma3:4b-conservative"
      - "llama3.2:1b" 